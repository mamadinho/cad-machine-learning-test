{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RJXTCntZfqU",
        "outputId": "0234d7e9-0224-4e3e-c706-e61dcc69b493"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device= torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-M_G-WCcTKY",
        "outputId": "87e49a96-983c-4c6a-ca71-2fcf69bb755f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "3EUpgm2taANp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "1Px03-Ercg8p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = 'asdasdsDADSDAS'.lower()\n",
        "txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Gtj6rumUrVx9",
        "outputId": "4e118f70-44ef-43b2-f574-c54b0427ef1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'asdasdsdadsdas'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = []\n",
        "  \n",
        "for f in os.listdir('dataset'):\n",
        "  file1 = open(f'dataset/{f}', 'r')\n",
        "  lines = file1.readlines()\n",
        "\n",
        "  # Strips the newline character\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "      if(line[:4] == \"MISC\"):\n",
        "        df.append([line[5:], 'MISC'])\n",
        "      elif(line[:4] == 'CONT'):\n",
        "        df.append([line[5:], 'CONT'])\n",
        "      elif(line[:4] == 'AIMX'):\n",
        "        df.append([line[5:], 'AIMX'])\n",
        "      elif(line[:4] == 'OWNX'):\n",
        "        df.append([line[5:], 'OWNX'])\n",
        "      elif(line[:4] == 'BASE'):\n",
        "        df.append([line[5:], 'BASE'])\n",
        "    # count += 1\n",
        "    # print(\"Line{}: {}\".format(count, line.strip()))"
      ],
      "metadata": {
        "id": "zoXTuBCnaIBH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(df, columns = ['text', 'label'])"
      ],
      "metadata": {
        "id": "0Q5Fj9m7aWCA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis"
      ],
      "metadata": {
        "id": "tYW0pZqBhnNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aimx = df[df.label == 'AIMX']\n",
        "aimx['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-uBNnVZhFl4",
        "outputId": "450bb244-618b-41c8-d427-5c25260b8961"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2       Here, we present the first genome-scale, fine-...\n",
              "5       Furthermore, it was converted into a mathemati...\n",
              "32      In this study, we present a new generation of ...\n",
              "45      Our study provides the atomic resolution descr...\n",
              "53      It will then be beneficial to understand the m...\n",
              "                              ...                        \n",
              "3056    Here, using computer simulations of isolated p...\n",
              "3079    we show that   can be studied by using algebra...\n",
              "3095    in this paper  we approach the problem in its ...\n",
              "3113    this note states two simple results about defe...\n",
              "3115    in our informal discussions we will be assumin...\n",
              "Name: text, Length: 194, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "tMGv3yVWdfmY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {\n",
        "    'MISC': 0,\n",
        "    'CONT': 1,\n",
        "    'AIMX': 2, \n",
        "    'OWNX': 3,\n",
        "    'BASE': 4\n",
        "}"
      ],
      "metadata": {
        "id": "ZDBbJPv3djK4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = [labels[label] for label in df['label']]\n",
        "        self.texts = [tokenizer(text, \n",
        "                               padding='max_length', max_length = 32, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in tqdm(df['text'], total=len(df))]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "    \n",
        "    def get_batch_variables(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.variables[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "15bO6aGBbkV9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(40)\n",
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
        "                                     [int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "print(len(df_train),len(df_val), len(df_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJjkrG16drWt",
        "outputId": "aaf82ec4-c47c-45ba-b56e-28fdc0952e9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2493 312 312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = Dataset(df_train), Dataset(df_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMreyRojduAt",
        "outputId": "2f82ec53-162a-4ae0-8b3e-155e51105382"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2493/2493 [00:02<00:00, 1210.79it/s]\n",
            "100%|██████████| 312/312 [00:00<00:00, 1326.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "h0yb6MCsdzWo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "EPOCHS = 20\n",
        "model = BertClassifier().cuda()\n",
        "LR = 5e-6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K485hHYweDin",
        "outputId": "bba82ceb-d3b7-4c76-a3d0-79b8e9b5f386"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "train_acc = []\n",
        "val_loss = []\n",
        "val_acc = []"
      ],
      "metadata": {
        "id": "FBZrP0Z5eFEh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_func(model, train_dataloader, val_dataloader, learning_rate, epochs):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "    \n",
        "    bs = 64\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "            total_data_train = 0\n",
        "            bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
        "            \n",
        "            for step, (train_input, train_label) in bar:\n",
        "\n",
        "                bar.set_description(f\"Epoch {epoch_num}\")\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(output, train_label.long())\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_data_train+=bs\n",
        "#                 bar.set_postfix(Epoch_Loss=total_loss_train/total_data_train, \n",
        "#                                 Batch_Loss=batch_loss.item())\n",
        "\n",
        "                total_acc_val = 0\n",
        "                total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label.long())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "\n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader.dataset): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_dataloader.dataset): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_dataloader.dataset): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}')\n",
        "            train_loss.append(total_loss_train / len(train_dataloader.dataset))\n",
        "            train_acc.append(total_acc_train / len(train_dataloader.dataset))\n",
        "            val_loss.append(total_loss_val / len(val_dataloader.dataset))\n",
        "            val_acc.append(total_acc_val / len(val_dataloader.dataset))"
      ],
      "metadata": {
        "id": "BR3_nY5Qefxx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val, batch_size=32)"
      ],
      "metadata": {
        "id": "5aFqc3ohen3l"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_func(model, train_dataloader, val_dataloader, LR, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4lfh0tbeuV8",
        "outputId": "9ff973e0-a236-4a55-ad81-1129ce255f1a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 78/78 [00:14<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.041                 | Train Accuracy:  0.521                 | Val Loss:  0.033                 | Val Accuracy:  0.663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 78/78 [00:14<00:00,  5.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.028                 | Train Accuracy:  0.717                 | Val Loss:  0.027                 | Val Accuracy:  0.747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 78/78 [00:14<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.021                 | Train Accuracy:  0.820                 | Val Loss:  0.023                 | Val Accuracy:  0.798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 78/78 [00:13<00:00,  5.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss:  0.018                 | Train Accuracy:  0.850                 | Val Loss:  0.021                 | Val Accuracy:  0.808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 78/78 [00:14<00:00,  5.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss:  0.015                 | Train Accuracy:  0.870                 | Val Loss:  0.019                 | Val Accuracy:  0.837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 78/78 [00:14<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 6 | Train Loss:  0.014                 | Train Accuracy:  0.874                 | Val Loss:  0.018                 | Val Accuracy:  0.833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 78/78 [00:14<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 7 | Train Loss:  0.012                 | Train Accuracy:  0.876                 | Val Loss:  0.018                 | Val Accuracy:  0.824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 78/78 [00:14<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 8 | Train Loss:  0.012                 | Train Accuracy:  0.878                 | Val Loss:  0.019                 | Val Accuracy:  0.821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 78/78 [00:14<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 9 | Train Loss:  0.011                 | Train Accuracy:  0.880                 | Val Loss:  0.018                 | Val Accuracy:  0.830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 78/78 [00:14<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 10 | Train Loss:  0.010                 | Train Accuracy:  0.886                 | Val Loss:  0.018                 | Val Accuracy:  0.833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 78/78 [00:14<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 11 | Train Loss:  0.010                 | Train Accuracy:  0.887                 | Val Loss:  0.018                 | Val Accuracy:  0.840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 78/78 [00:14<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 12 | Train Loss:  0.009                 | Train Accuracy:  0.892                 | Val Loss:  0.018                 | Val Accuracy:  0.843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 78/78 [00:14<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 13 | Train Loss:  0.009                 | Train Accuracy:  0.897                 | Val Loss:  0.018                 | Val Accuracy:  0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 78/78 [00:14<00:00,  5.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 14 | Train Loss:  0.008                 | Train Accuracy:  0.901                 | Val Loss:  0.019                 | Val Accuracy:  0.837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 78/78 [00:14<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 15 | Train Loss:  0.008                 | Train Accuracy:  0.904                 | Val Loss:  0.018                 | Val Accuracy:  0.846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 78/78 [00:14<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 16 | Train Loss:  0.007                 | Train Accuracy:  0.905                 | Val Loss:  0.018                 | Val Accuracy:  0.840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 78/78 [00:14<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 17 | Train Loss:  0.007                 | Train Accuracy:  0.909                 | Val Loss:  0.019                 | Val Accuracy:  0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 78/78 [00:14<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 18 | Train Loss:  0.007                 | Train Accuracy:  0.906                 | Val Loss:  0.019                 | Val Accuracy:  0.853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 78/78 [00:14<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 19 | Train Loss:  0.007                 | Train Accuracy:  0.909                 | Val Loss:  0.018                 | Val Accuracy:  0.846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 78/78 [00:14<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 20 | Train Loss:  0.007                 | Train Accuracy:  0.905                 | Val Loss:  0.018                 | Val Accuracy:  0.837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report"
      ],
      "metadata": {
        "id": "qkHs8Rx2hJGD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(s):\n",
        "    np_array = np.array([labels[s.label]])\n",
        "    label = torch.from_numpy(np_array)\n",
        "    train_label = label.to(device)\n",
        "\n",
        "    true = train_label.item()\n",
        "        \n",
        "    text = tokenizer(s.text, padding='max_length', max_length = 32, truncation=True,\n",
        "                            return_tensors=\"pt\")\n",
        "    \n",
        "    mask = text['attention_mask'].to(device)\n",
        "    input_id = text['input_ids'].squeeze(1).to(device)\n",
        "    \n",
        "    preds = []\n",
        "    output = model(input_id, mask)\n",
        "    preds = torch.argmax(output).cpu().detach().numpy()\n",
        "\n",
        "    return preds, true"
      ],
      "metadata": {
        "id": "Sf0fOnIjmWVW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "T = 0\n",
        "F = 0\n",
        "\n",
        "for i, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
        "    p, t = predict(row)\n",
        "    y_true.append(t)\n",
        "    y_pred.append(p)\n",
        "    if(t in p):\n",
        "        T+=1\n",
        "    else:\n",
        "        F+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYEntrDnmPnY",
        "outputId": "83adcd8b-eeee-4aee-e0f3-20215c55012d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 312/312 [00:03<00:00, 81.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyuOurnlmiGe",
        "outputId": "d4e30fe3-05b0-446f-cc73-43a2e4adbf9e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.94      0.90       175\n",
            "           1       0.81      0.59      0.68        22\n",
            "           2       0.85      0.52      0.65        21\n",
            "           3       0.76      0.79      0.77        84\n",
            "           4       0.75      0.30      0.43        10\n",
            "\n",
            "    accuracy                           0.83       312\n",
            "   macro avg       0.81      0.63      0.69       312\n",
            "weighted avg       0.82      0.83      0.82       312\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With weights"
      ],
      "metadata": {
        "id": "f0aZ1Wkn1izT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "EPOCHS = 20\n",
        "model = BertClassifier().cuda()\n",
        "LR = 5e-6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_UjqFY_2uFJ",
        "outputId": "9ef66205-3ca6-4251-9345-50f3cb9c9e41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=df.label.unique(), y=df_train.label)\n",
        "class_weights = np.array(class_weights).astype(np.float32)\n",
        "for i in range(len(class_weights)):\n",
        "    x = class_weights[i]\n",
        "    if(x < 0.5):\n",
        "        class_weights[i] = 0.5\n",
        "    elif(x > 10):\n",
        "        class_weights[i] = 10\n",
        "class_weights = torch.tensor(class_weights)"
      ],
      "metadata": {
        "id": "Vw0JmeH61gND"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db72a27-2b53-42f0-eb32-62fa0b915593",
        "id": "Kzm8AylQ1gNE"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.5000,  3.8061,  3.2168,  0.7143, 10.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_func(model, train_dataloader, val_dataloader, learning_rate, epochs):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "    \n",
        "    bs = 64\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "            total_data_train = 0\n",
        "            bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
        "            \n",
        "            for step, (train_input, train_label) in bar:\n",
        "\n",
        "                bar.set_description(f\"Epoch {epoch_num}\")\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(output, train_label.long())\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_data_train+=bs\n",
        "#                 bar.set_postfix(Epoch_Loss=total_loss_train/total_data_train, \n",
        "#                                 Batch_Loss=batch_loss.item())\n",
        "\n",
        "                total_acc_val = 0\n",
        "                total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label.long())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "\n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "\n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader.dataset): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_dataloader.dataset): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_dataloader.dataset): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}')\n",
        "            train_loss.append(total_loss_train / len(train_dataloader.dataset))\n",
        "            train_acc.append(total_acc_train / len(train_dataloader.dataset))\n",
        "            val_loss.append(total_loss_val / len(val_dataloader.dataset))\n",
        "            val_acc.append(total_acc_val / len(val_dataloader.dataset))"
      ],
      "metadata": {
        "id": "ehmWqy8c1gNF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val, batch_size=32)"
      ],
      "metadata": {
        "id": "p6xU4gdN1gNF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_func(model, train_dataloader, val_dataloader, LR, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de3c660-ba69-48b1-e086-47626c995eb0",
        "id": "CAVwGHvB1gNF"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 78/78 [00:14<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.050                 | Train Accuracy:  0.315                 | Val Loss:  0.051                 | Val Accuracy:  0.388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 78/78 [00:14<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.047                 | Train Accuracy:  0.530                 | Val Loss:  0.047                 | Val Accuracy:  0.628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 78/78 [00:14<00:00,  5.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.043                 | Train Accuracy:  0.662                 | Val Loss:  0.042                 | Val Accuracy:  0.679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 78/78 [00:14<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss:  0.035                 | Train Accuracy:  0.726                 | Val Loss:  0.038                 | Val Accuracy:  0.708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 78/78 [00:14<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss:  0.029                 | Train Accuracy:  0.774                 | Val Loss:  0.033                 | Val Accuracy:  0.766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 78/78 [00:14<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 6 | Train Loss:  0.024                 | Train Accuracy:  0.839                 | Val Loss:  0.031                 | Val Accuracy:  0.776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 78/78 [00:14<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 7 | Train Loss:  0.020                 | Train Accuracy:  0.860                 | Val Loss:  0.029                 | Val Accuracy:  0.811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 78/78 [00:14<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 8 | Train Loss:  0.017                 | Train Accuracy:  0.874                 | Val Loss:  0.030                 | Val Accuracy:  0.804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 78/78 [00:14<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 9 | Train Loss:  0.015                 | Train Accuracy:  0.886                 | Val Loss:  0.028                 | Val Accuracy:  0.827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 78/78 [00:14<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 10 | Train Loss:  0.012                 | Train Accuracy:  0.894                 | Val Loss:  0.027                 | Val Accuracy:  0.821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 78/78 [00:14<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 11 | Train Loss:  0.011                 | Train Accuracy:  0.900                 | Val Loss:  0.027                 | Val Accuracy:  0.821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 78/78 [00:14<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 12 | Train Loss:  0.010                 | Train Accuracy:  0.903                 | Val Loss:  0.028                 | Val Accuracy:  0.833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 78/78 [00:14<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 13 | Train Loss:  0.009                 | Train Accuracy:  0.907                 | Val Loss:  0.029                 | Val Accuracy:  0.833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 78/78 [00:14<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 14 | Train Loss:  0.009                 | Train Accuracy:  0.909                 | Val Loss:  0.029                 | Val Accuracy:  0.830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 78/78 [00:14<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 15 | Train Loss:  0.008                 | Train Accuracy:  0.909                 | Val Loss:  0.030                 | Val Accuracy:  0.824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 78/78 [00:14<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 16 | Train Loss:  0.007                 | Train Accuracy:  0.914                 | Val Loss:  0.029                 | Val Accuracy:  0.827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 78/78 [00:14<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 17 | Train Loss:  0.007                 | Train Accuracy:  0.914                 | Val Loss:  0.030                 | Val Accuracy:  0.833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 78/78 [00:14<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 18 | Train Loss:  0.007                 | Train Accuracy:  0.914                 | Val Loss:  0.033                 | Val Accuracy:  0.827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 78/78 [00:14<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 19 | Train Loss:  0.007                 | Train Accuracy:  0.915                 | Val Loss:  0.029                 | Val Accuracy:  0.824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 78/78 [00:14<00:00,  5.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 20 | Train Loss:  0.007                 | Train Accuracy:  0.913                 | Val Loss:  0.032                 | Val Accuracy:  0.830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report"
      ],
      "metadata": {
        "id": "xOoE7iQc1gNG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(s):\n",
        "    np_array = np.array([labels[s.label]])\n",
        "    label = torch.from_numpy(np_array)\n",
        "    train_label = label.to(device)\n",
        "\n",
        "    true = train_label.item()\n",
        "        \n",
        "    text = tokenizer(s.text, padding='max_length', max_length = 32, truncation=True,\n",
        "                            return_tensors=\"pt\")\n",
        "    \n",
        "    mask = text['attention_mask'].to(device)\n",
        "    input_id = text['input_ids'].squeeze(1).to(device)\n",
        "    \n",
        "    preds = []\n",
        "    output = model(input_id, mask)\n",
        "    preds = torch.argmax(output).cpu().detach().numpy()\n",
        "\n",
        "    return preds, true"
      ],
      "metadata": {
        "id": "1CbHizTh1gNG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "T = 0\n",
        "F = 0\n",
        "\n",
        "for i, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
        "    p, t = predict(row)\n",
        "    y_true.append(t)\n",
        "    y_pred.append(p)\n",
        "    if(t in p):\n",
        "        T+=1\n",
        "    else:\n",
        "        F+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cee9095-42ee-44e4-a9e4-050ae83b658a",
        "id": "BW8KVj4S1gNG"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 312/312 [00:03<00:00, 82.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6b0ef8-39b4-4007-be79-e0fedea8bc80",
        "id": "8OswK2z51gNG"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       175\n",
            "           1       0.73      0.73      0.73        22\n",
            "           2       0.84      0.76      0.80        21\n",
            "           3       0.80      0.79      0.80        84\n",
            "           4       0.86      0.60      0.71        10\n",
            "\n",
            "    accuracy                           0.85       312\n",
            "   macro avg       0.82      0.76      0.79       312\n",
            "weighted avg       0.85      0.85      0.85       312\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "om3yPWXs10sp"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}